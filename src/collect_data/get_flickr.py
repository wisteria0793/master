import flickrapi
import json
import urllib.request
import os
import configparser
import datetime
import time 
import sys
import pprint
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦èª­ã¿è¾¼ã‚€
load_dotenv('.env')




# --- 1. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã¨ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ ---


# --- 1. å®šæ•°ã¨è¨­å®š ---
HAKODATE_BBOX = '140.5,41.7,141.0,41.9' 
PER_PAGE = 500  
DOWNLOAD_DIR = './data/raw/hakodate_all_photos'
MAX_RETRIES = 3         
START_PAGE_NUMBER = 1   

# âœ… ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã®ã‚³ã‚¢è¨­å®š: 1æ™‚é–“3600å› (1å›/ç§’) ã®åˆ¶é™ã‚’ç¢ºå®Ÿã«ä¸‹å›ã‚‹ãŸã‚ã®é–“éš”
RATE_LIMIT_DELAY = 1.05 
INNER_LOOP_DELAY = RATE_LIMIT_DELAY # å†™çœŸ1æšã‚ãŸã‚Šã®å¾…æ©Ÿæ™‚é–“ã¨ã—ã¦é©ç”¨


api_key = os.getenv('FLICKR_API_KEY')
api_secret = os.getenv('FLICKR_SECRET_KEY')

    
flickr = flickrapi.FlickrAPI(api_key, api_secret, format='json') 

# --- 3. ãƒ•ã‚©ãƒ«ãƒ€ã®æº–å‚™ ---
if not os.path.exists(DOWNLOAD_DIR):
    os.makedirs(DOWNLOAD_DIR)

print(f"--- å‡½é¤¨å¸‚å†…ï¼ˆbbox: {HAKODATE_BBOX}ï¼‰ã®å…¨ä»¶åé›†ã‚’é–‹å§‹ã—ã¾ã™ ---")

# ==============================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ==============================================================================

def get_photo_comments(photo_id):
    """æŒ‡å®šã•ã‚ŒãŸå†™çœŸIDã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã€JSONä¿å­˜ç”¨ã®ãƒªã‚¹ãƒˆã§è¿”ã—ã¾ã™ã€‚"""
    
    # âœ… ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–: ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—APIã‚³ãƒ¼ãƒ«ç›´å‰ã«å¾…æ©Ÿ
    time.sleep(RATE_LIMIT_DELAY) 
    
    try:
        comments_response = flickr.photos.comments.getList(photo_id=photo_id)
        comments_data = json.loads(comments_response.decode('utf-8'))
        
        comment_count = 0
        comment_list = []

        if comments_data['comments'].get('comment'):
            comments = comments_data['comments']['comment']
            if not isinstance(comments, list): comments = [comments]
            comment_count = len(comments)
            for comment in comments:
                comment_list.append({
                    'author_name': comment['authorname'],
                    'content': comment['_content']
                })
        
        print(f"       ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—æˆåŠŸ: {comment_count}ä»¶ (é–“éš” {RATE_LIMIT_DELAY}ç§’)")
        return comment_list
    except Exception as e:
        print(f"       ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—å¤±æ•—: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({e})")
        return []


def fetch_page_data(current_page):
    """Flickr APIã‹ã‚‰ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€å¤±æ•—æ™‚ã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™ã€‚"""
    
    params = {
        'bbox': HAKODATE_BBOX,
        'per_page': PER_PAGE,
        'page': current_page,
        'has_geo': 1,
        'safe_search': 1,
        'extras': 'url_m,date_taken,owner_name,geo,tags,secret,description,views,date_upload,count_faves',
        'sort': 'date-posted-desc'
    }
    
    for attempt in range(MAX_RETRIES):
        try:
            # âœ… ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–: ãƒšãƒ¼ã‚¸å–å¾—APIã‚³ãƒ¼ãƒ«ç›´å‰ã«å¾…æ©Ÿ (åˆå›ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚ã‚Š)
            if current_page > 1 or attempt > 0:
                time.sleep(RATE_LIMIT_DELAY) 
                
            response = flickr.photos.search(**params)
            data = json.loads(response.decode('utf-8'))
            
            if data.get('stat') == 'fail':
                raise Exception(f"Flickr API Status Fail: {data.get('message', 'Unknown Error')}")

            print(f"   [API SUCCESS] ãƒšãƒ¼ã‚¸ {current_page} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
            return data 
            
        except Exception as e:
            print(f"\nâš ï¸ ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (ãƒšãƒ¼ã‚¸ {current_page}, è©¦è¡Œ {attempt + 1}/{MAX_RETRIES}): {e}")
            
            if attempt < MAX_RETRIES - 1:
                wait_time = 5 * (attempt + 1)
                # ã‚¨ãƒ©ãƒ¼æ™‚ã®å¾…æ©Ÿæ™‚é–“ã¯ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã¨ã¯åˆ¥ã§ã€ã‚µãƒ¼ãƒãƒ¼å›å¾©ã‚’å¾…ã¤æ™‚é–“
                print(f"   {wait_time}ç§’å¾…æ©Ÿã—ã¦ã‹ã‚‰ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                time.sleep(wait_time)
            else:
                print(f"   ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’è¶…éã—ã¾ã—ãŸã€‚ã“ã®ãƒšãƒ¼ã‚¸ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return None
    return None


# ==============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç† (å…¨ä»¶åé›†ãƒ­ã‚¸ãƒƒã‚¯)
# ==============================================================================
try:
    # --- 4. æœ€åˆã®æ¤œç´¢: ç·æšæ•°ã¨ç·ãƒšãƒ¼ã‚¸æ•°ã‚’å–å¾— ---
    # åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ RATE_LIMIT_DELAY ã®é©ç”¨å¤–ã¨ã—ã¦ã€ã™ãã«å®Ÿè¡Œã‚’è©¦ã¿ã‚‹ (fetch_page_dataå†…ã§å¾…æ©Ÿã™ã‚‹å¯èƒ½æ€§ã‚ã‚Š)
    initial_data = fetch_page_data(1) 
    
    if initial_data is None:
        print("åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        sys.exit(1)

    photos_meta = initial_data['photos']
    total_photos = int(photos_meta['total'])
    total_pages = int(photos_meta['pages'])
    
    print(f"ğŸ’¡ åˆè¨ˆ {total_photos} ä»¶ã®å†™çœŸãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ï¼ˆå…¨ {total_pages} ãƒšãƒ¼ã‚¸ï¼‰")
    if START_PAGE_NUMBER > 1:
         print(f"âœ… å‡¦ç†ã‚’ãƒšãƒ¼ã‚¸ {START_PAGE_NUMBER} ã‹ã‚‰å†é–‹ã—ã¾ã™ã€‚\n")
    else:
         print("\n")

    # --- ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ«ãƒ¼ãƒ— ---
    for current_page in range(START_PAGE_NUMBER, total_pages + 1):
        
        # å‡¦ç†ã™ã‚‹å†™çœŸãƒªã‚¹ãƒˆã‚’å–å¾—
        if current_page == 1:
            current_photos = photos_meta['photo']
        else:
            print(f"   --> ãƒšãƒ¼ã‚¸ {current_page}/{total_pages} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
            # fetch_page_dataå†…ã§æ—¢ã«å¾…æ©Ÿå‡¦ç†ãŒå«ã¾ã‚Œã¦ã„ã‚‹
            page_data = fetch_page_data(current_page)
            
            if page_data is None:
                continue 
            current_photos = page_data['photos']['photo']
        
        
        print(f"   --- ãƒšãƒ¼ã‚¸ {current_page}/{total_pages} ã‚’å‡¦ç†ä¸­ ({len(current_photos)}æš) ---")
        
        # --- å†™çœŸã”ã¨ã®è©³ç´°å‡¦ç†ï¼ˆJSONä¿å­˜ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰ ---
        for i, photo in enumerate(current_photos):
            photo_id = photo['id']
            image_url = photo.get('url_m')
            
            # --- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åé›†ã¨æ•´å½¢ ---
            date_upload_unix = photo.get('dateupload', 'N/A')
            date_upload_readable = 'N/A'
            if date_upload_unix != 'N/A' and date_upload_unix.isdigit():
                date_upload_readable = datetime.datetime.fromtimestamp(int(date_upload_unix)).strftime('%Y-%m-%d %H:%M:%S')

            # ã‚³ãƒ¡ãƒ³ãƒˆã®å–å¾— (APIå‘¼ã³å‡ºã— #2) - ã“ã®é–¢æ•°å†…ã§ RATE_LIMIT_DELAY ãŒå®Ÿè¡Œã•ã‚Œã‚‹
            comments_list = get_photo_comments(photo_id) 

            # JSONæ ¼ç´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ (çœç•¥)
            photo_metadata = {
                'id': photo_id, 'secret': photo.get('secret', 'N/A'),
                'title': photo.get('title', 'N/A'), 'owner_name': photo.get('ownername', 'N/A'),
                'url_page': f"https://www.flickr.com/photos/{photo['owner']}/{photo_id}", 'url_image_m': image_url,
                'datetime': {'taken': photo.get('datetaken', 'N/A'), 'uploaded_unix': date_upload_unix, 'uploaded_readable': date_upload_readable,},
                'stats': {'views': photo.get('views', '0'), 'faves': photo.get('count_faves', '0'),},
                'location': {'latitude': photo.get('latitude', 'N/A'), 'longitude': photo.get('longitude', 'N/A'), 'accuracy': photo.get('accuracy', 'N/A'),},
                'tags': photo.get('tags', 'N/A').split(' '), 'description': photo.get('description', {}).get('_content', 'N/A'),
                'comments': comments_list
            }

            # ç°¡æ½”ãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
            print(f"     [{i+1}/{len(current_photos)}] ID: {photo_id[:10]}... | Title: {photo.get('title', 'N/A')[:30]}...")

            # --- JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ ---
            json_filename = os.path.join(DOWNLOAD_DIR, f"{photo_id}.json")
            if not os.path.exists(json_filename): 
                try:
                    with open(json_filename, 'w', encoding='utf-8') as f:
                        json.dump(photo_metadata, f, ensure_ascii=False, indent=4)
                    print(f"       âœ… JSONä¿å­˜: æˆåŠŸ")
                except Exception as json_e:
                    print(f"       âŒ JSONä¿å­˜: å¤±æ•— ({json_e})")
            else:
                print(f"       â¡ï¸ JSONä¿å­˜: ã‚¹ã‚­ãƒƒãƒ— (ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨)")

            
            # --- ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ---
            if image_url:
                image_filename = os.path.join(DOWNLOAD_DIR, f"{photo_id}_{photo.get('secret', 'no_secret')}.jpg")
                if not os.path.exists(image_filename): 
                    try:
                        urllib.request.urlretrieve(image_url, image_filename)
                        print(f"       âœ… ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: æˆåŠŸ")
                    except Exception as dl_e:
                        print(f"       âŒ ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: å¤±æ•— ({dl_e})")
                else:
                    print(f"       â¡ï¸ ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: ã‚¹ã‚­ãƒƒãƒ— (ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨)")
            
            # ãƒšãƒ¼ã‚¸ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ã‚³ãƒ¡ãƒ³ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã®é–“ã«ã€è¿½åŠ ã®APIã‚³ãƒ¼ãƒ«ãŒãªã„ãŸã‚ã€
            # ã“ã®ä½ç½®ã§ã® sleep ã¯ä¸è¦ã«ãªã‚Šã¾ã—ãŸã€‚sleep ã¯ API ã‚³ãƒ¼ãƒ«ã®ç›´å‰ã§è¡Œã‚ã‚Œã¾ã™ã€‚
            pass 
        
    print(f"\nâœ… å…¨ {total_photos} ä»¶ã®å†™çœŸã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

except Exception as e:
    print(f"\nâŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")